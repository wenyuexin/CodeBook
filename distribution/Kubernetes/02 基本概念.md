# K8s基本概念

Kubernetes中的大部分概念如Node、Pod、Replication Controller、Service等都可以被看作一种资源对象，几乎所有资源对象都可以通过Kubernetes提供的kubectl工具（或者API编程调用）执行增、删、改、查等操作并将其保存在etcd中持久化存储。

[TOC]

Master, Node, Pod, Label, Replication Controller, Deployment, Horizontal Pod Autoscaler, StatefulSet, Service, Job, Volume,  Persistent Volume, Namespace, Annotation, ConfigMap

## 1 Master

Kubernetes里的Master指的是集群控制节点，在每个K8s集群里都需要有一个Master来负责整个集群的管理和控制，基本上K8s的所有控制命令都发给它，它负责具体的执行过程。

Master通常会占据一个独立的服务器（高可用部署建议用3台服务器），如果它宕机或者不可用，那么对集群内容器应用的管理都将失效。

在Master上运行着以下关键进程：

- Kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程）
- Kubernetes Controller Manager（kube-controller-manager）：所有资源对象的自动化控制中心
- Kubernetes Scheduler（kube-scheduler）：负责资源调度的进程。

## 2 Node

除了Master，Kubernetes集群中的其他机器被称为Node.

与Master一样，Node可以是一台物理主机，也可以是一台虚拟机。

Node是K8s集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。

在每个Node上都运行着以下关键进程：

- kubelet：负责Pod对应的容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能。
- kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件。
- Docker Engine（docker）：Docker引擎，负责本机的容器创建和管理工作

Node可以在运行期间动态增加到Kubernetes集群中，前提是在这个节点上已经正确安装、配置和启动了上述关键进程。在默认情况下kubelet会向Master注册自己。

一旦Node被纳入集群管理范围，kubelet进程就会定时向Master汇报自身的信息。Master根据每个Node的资源使用情况，并实现高效均衡的资源调度策略。超过指定时间不上报信息时，会被Master判定为“失联”，Node的状态被标记为不可用（Not Ready）。

## 3 Pod

Pod是Kubernetes最重要的基本概念，是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。

在组成上，每个Pod都有一个特殊的被称为“根容器”的Pause容器。此外，每个Pod还包含一个或多个紧密相关的用户业务容器。Pause容器与业务无关，其状态可以代表整个容器组的状态，用于判断整体是否死亡。其次，共享Pause容器挂接的Volume，可以简化业务容器之间的通信问题，并解决业务容器之间的文件共享问题。

Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。

Pod有两种类型：普通的Pod及静态Pod（Static Pod）。后者比较特殊，它并没被存放在Kubernetes的etcd存储里，而是被存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动、运行。普通的Pod一旦被创建，就会被放入etcd中存储，随后会被Master调度到某个具体的Node上并进行绑定（Binding），随后该Pod被
对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动。

默认情况下，当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod重新调度到其他节点上

> Pod、容器与Node的关系，简单来说就是有好多物理主机，其中一台（或者更多台）作为Master并负责调度，其他的是Node，Node里有若干Pod，每个Pod里装了一组业务相关的容器（例如docker容器）。

Docker Volume在Kubernetes里也有对应的概念—Pod Volume，后者有一些扩展，比如可以用分布式文件系统GlusterFS实现后端存储功能；Pod Volume是被定义在Pod上，然后被各个容器挂载到自己的文件系统中。

Event是一个事件的记录，记录了事件的最早产生时间、最后重现时间、重复次数、发起者、类型，以及导致此事件的原因等众多信息。Event通常会被关联到某个具体的资源对象上，是排查故障的重要参考信息。

在Kubernetes里，一个计算资源进行配额限定时需要设定以下两个参数：

- Requests：该资源的最小申请量，系统必须满足要求。
- Limits：该资源最大允许使用的量，不能被突破，当容器试图使用超过这个量的资源时，可能会被K8s杀掉并重启。

## 4 Label —— 标签

Label（标签）是一个key=value的键值对，其中key与value由用户自己指定。

Label可以被附加到各种资源对象上，例如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。

常见的Label有：版本标签release（stable、canary等）、环境标签environment、架构标签tier（frontend、backend、middleware等）、分区标签partition、质量管控标签track（daily、weekly）等等。

可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实
现了类似SQL的简单又通用的对象查询机制。Selector可以使用等式（=, !=）、集合两种表达方式（in, notin）。matchLabels用于定义一组Label，与直接写在Selector中的作用相同；matchExpressions用于定义一组基于集合的筛选条件，可用的条件运算符包括In、NotIn、Exists和DoesNotExist。例如：

```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

## 5 Annotation —— 注解

You can use Kubernetes annotations to **attach arbitrary non-identifying metadata to objects**. Clients such as tools and libraries can retrieve this metadata.

Annotation与Label类似，也使用key/value键值对的形式进行定义。不同的是Label具有严格的命名规则，它定义的是Kubernetes对象的元数据（Metadata），并且用于Label Selector。Annotation是用户任意定义的附加信息，以便于外部工具查找。在很多时候，Kubernetes的模块自身会通过Annotation标记资源对象的一些特殊信息。

通常来说，用Annotation来记录的信息如下。

- build信息、release信息、Docker镜像信息等，例如时间戳、release id号、PR号、镜像Hash值、Docker Registry地址等。

- 日志库、监控库、分析库等资源库的地址信息。

- 程序调试工具信息，例如工具名称、版本号等。

- 团队的联系信息，例如电话号码、负责人名称、网址等。

例1

```
"metadata": {
  "annotations": {
    "key1" : "value1",
    "key2" : "value2"
  }
}
```

例2

```
apiVersion: v1
kind: Pod
metadata:
  name: annotations-demo
  annotations:
    imageregistry: "https://hub.docker.com/"
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

## 6 Replication Controller（RC）

Replication Controller（简称RC），定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，所以RC的定义包括如下几个部分。

- Pod期待的副本数量。
- 用于筛选目标Pod的Label Selector。
- 当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板（template）

简单来说，RC的作用就是**确保Pod以你指定的副本数运行**。

定义了一个RC并将其提交到Kubernetes集群中后，Master上的Controller Manager组件就得到通知，定期巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，如果有过多的Pod副本在运行，系统就会停掉一些Pod，否则系统会再自动创建一些Pod。

例如：

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3 （这里规定了要有3个对应的Pod在运行）
  selector:
    app: nginx （selector限定了哪些标签的Pod受到RC控制）
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

删除RC并不会影响通过该RC已创建好的Pod。为了删除所有Pod，可以设置replicas的值为0，然后更新该RC。另外，kubectl提供了stop和delete命令来一次性删除RC和RC控制的全部Pod。

在Kubernetes 1.2中，RC升级为`ReplicaSet`，主要区别在于ReplicaSet可以通过集合语言限定Pod，而RC仅支持等式。

此外，当前很少单独使用ReplicaSet，它主要被`Deployment`这个更高层的资源对象所使用，从而形成一整套Pod创建、删除、更新的编排机制。我们在使用Deployment时，无须关心它是如何创建和维护ReplicaSet的，这一切都是自动发生的。

## 7 Deployment

Deployment在内部使用了Replica Set来实现，用于更好地解决Pod的编排问题。

Deployment相对于RC的一大升级是可以随时知道当前Pod的部署进度。由于一个Pod的创建、调度、绑定节点及在目标Node上启动对应的容器这一完整过程需要一定的时间，所以系统启动N个Pod副本，实际上是一个连续变化的部署过程。

Deployment的**典型使用场景**有以下几个：

- 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建。
- 检查Deployment的状态来看部署动作是否完成（Pod副本数量是否达到预期的值）。
- 更新Deployment以创建新的Pod（比如镜像升级）。
- 如果当前Deployment不稳定，则回滚到一个早先的Deployment版本。
- 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再恢复Deployment，进行新的发布。
- 扩展Deployment以应对高负载。
- 查看Deployment的状态，以此作为发布是否成功的指标。
- 清理不再需要的旧版本ReplicaSets。

Deployment的定义与ReplicaSet的定义很类似：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

Pod的管理对象，除了RC和Deployment，还包括ReplicaSet、DaemonSet、StatefulSet、Job等，分别用于不同的应用场景。

## 8 Horizontal Pod Autoscaler（Pod横向自动扩容，HPA）

可以手动执行kubectl scale命令，进而实现Pod扩容或缩容。但是，根据Kubernetes的自动化、智能化的定位目标，分布式系统要能够根据当前负载的变化自动触发水平扩容或缩容，因为这一过程可能是频繁发生的、不可预料的，所以手动控制的方式是不现实的。

HPA与之前的RC、Deployment一样，也属于一种Kubernetes资源对象。通过追踪分析指定RC控制的所有目标Pod的负载变化情况，来确定是否需要有针对性地调整目标Pod的副本数量，这是HPA的实现原理。

当前，HPA有以下两种方式作为Pod负载的度量指标：

- CPUUtilizationPercentage，即目标Pod所有副本自身的CPU利用率的平均值。

  一个Pod自身的CPU利用率是该Pod当前CPU的使用量除以它的Pod Request的值；

  如果目标Pod没有定义Pod Request的值，则无法使用CPUUtilizationPercentage实现Pod横向自动扩容。

  在CPUUtilizationPercentage计算过程中使用到的Pod的CPU使用量通常是1min内的平均值。

  K8s自身孵化了一个基础性能数据采集监控框架——Kubernetes Monitoring Architecture。

- 应用程序自定义的度量指标，比如服务在每秒内的相应请求数（TPS或QPS）。

示例：

```
apiVersion: autoscaling/vl
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  maxReplicas: 10 （Pod的副本数为1～10）
  minReplicas: 1
  scaleTargetRef:
    kind: Deployment （HPA控制的目标对象是一个名为php-apache的Deployment里的Pod副本）
    name: php-apache
  targetCPUUtilizationPercentage: 90 （超过90%时会触发自动动态扩容行为）
```

## 9 StatefulSet

在Kubernetes系统中，Pod的管理对象RC、Deployment、DaemonSet和Job都面向无状态的服务。但现实中有很多服务是有状态的，特别是一些复杂的中间件集群，例如MySQL集群、MongoDB集群、ZooKeeper集群、Akka集群等，这些应用集群有4个共同点。

- 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信。
- 集群的规模是比较固定的，集群规模不能随意变动。
- 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中。
- 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损。

如果通过RC或Deployment控制Pod副本数量来实现上述有状态的集群，就会发现第1点是无法满足的，因为Pod的名称是随机产生的，Pod的IP地址也是在运行期才确定且可能有变动的。

StatefulSet从本质上来说，可以看作Deployment/RC的一个特殊变种，它有如下特性：

- **StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员**。假设StatefulSet的名称为kafka，那么第1个Pod叫kafka-0，第2个叫kafka-1，以此类推。
- StatefulSet控制的Pod副本的**启停顺序是受控的**，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态。
- StatefulSet里的Pod采用稳定的**持久化存储卷**，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷（为了保证数据的安全）

StatefulSet除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于哪个Headless Service。

## 10 Service

Service其实就是微服务架构中的一个微服务，之前讲解Pod、RC等资源对象其实都是为讲解Kubernetes Service做铺垫的。

Service定义了一个服务的访问入口地址，前端的应用（Frontend Pod）通过这个入口地址访问其背后的一组由
Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终符合预期标准。

Pod、RC与Service的逻辑关系：

```
(Frontend Pod) --> (Service) --> (Label Selector) ----> (Pod label:xxx)
                                         ↑        └---> (Pod label:xxx)
              (replicationController) ----┘        └---> (Pod label:xxx)
```

每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod IP+ContainerPort）以被客户端访问，并且多个Pod副本组成了一个集群来提供服务。

至于客户端如何来访问它们，一般的做法是部署一个**负载均衡器**（软件或硬件），为这组Pod开启一个对外的服务端口如8000端口，并且将这些Pod的Endpoint列表加入8000端口的转发列表，客户端就可以通过负载均衡器的对外IP地址以及服务端口来访问此服务。客户端的请求最后会被转发到哪个Pod，由负载均衡器的算法所决定。

运行在每个Node上的kube-proxy进程其实就是一个智能的软件负载均衡器，负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。但Kubernetes发明了一种很巧妙又影响深远的设计：**Service没有共用一个负载均衡器的IP地址，每个Service都被分配了一个全局唯一的虚拟IP地址**，这个虚拟IP被称为`Cluster IP`。这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网络通信问题。

Pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新Pod的IP地址与之前旧Pod的不同。而Service一旦被创建，Kubernetes就会自动为它分配一个可用的Cluster IP，而且在Service的整个生命周期内，它的Cluster IP不会发生改变。

例：

```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp （拥有app=MyApp标签的所有Pod都属于这个Service）
  ports:
    - protocol: TCP
      port: 80 （port属性则定义了Service的虚端口，此处服务端口为80）
      targetPort: 9376 （targetPort属性用来确定提供该服务的容器所暴露的端口号，未指定时与port相同）
```



## 11 Job —— 批处理任务的资源对象

批处理任务通常并行（或者串行）启动多个计算进程去处理一批工作项（work item），在处理完成后，整个批处理任务结束。

可以通过Kubernetes Job这种资源对象，定义并启动一个批处理任务Job。

异同：与RC、Deployment、ReplicaSet、DaemonSet类似，Job也控制一组Pod容器。同时Job控制Pod副本与RC等控制器的工作机制有以下重要差别：

- Job所控制的Pod副本是短暂运行的，**可以将其视为一组Docker容器，其中的每个Docker容器都仅运行一次**。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与RC等副本控制器不同，**Job生成的Pod副本是不能自动重启的**，对应Pod副本的RestartPoliy都被设置为Never。因此，当对应的Pod副本都执行完成时，相应的Job也就完成了控制使命，即Job生成的Pod在Kubernetes中是短暂存在的。

  Kubernetes在1.5版本之后又提供了类似crontab的定时任务——CronJob，解决了某些批处理任务需要定时反复执行的问题。

- Job所控制的Pod副本的工作模式能够多实例并行计算，以TensorFlow框架为例，可以将一个机器学习的计算任务分布到10台机器上，在每台机器上都运行一个worker执行计算任务，这很适合通过Job生成10个Pod副本同时启动运算。

## 12 Volume —— 存储卷

**Volume是Pod中能够被多个容器访问的共享目录。**

容器中的文件在磁盘上是临时存放的，这给容器中运行的特殊应用程序带来一些问题。 首先，当容器崩溃时，kubelet 将重新启动容器，容器中的文件将会丢失——因为容器会以干净的状态重建。 其次，当在一个 `Pod` 中同时运行多个容器时，常常需要在这些容器之间共享文件。 Kubernetes 抽象出 `Volume` 对象来解决这两个问题。

Kubernetes的Volume概念、用途和目的与Docker的Volume比较类似，但两者不能等价。第一，Kubernetes中的Volume被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；第二，Kubernetes中的
Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume中的数据也不会丢失；第三，Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等先进的分布式文件系统。

在使用上，通常先在Pod上声明一个Volume，然后在容器里引用该Volume并挂载（Mount）到容器里的某个目录上。使用卷时, Pod 声明中需要提供卷挂载的位置 (`.spec.containers.volumeMounts` 字段)和卷的类型 (`.spec.volumes` 字段)。

卷的核心是包含一些数据的目录，Pod 中的容器可以访问该目录。 特定的卷类型可以决定这个目录如何形成的，并能决定它支持何种介质，以及目录中存放什么内容。

容器中的进程能看到由它们的Docker镜像和卷组成的文件系统视图。 Docker镜像位于文件系统层次结构的根部，并且任何 Volume 都挂载在镜像内的指定路径上。 卷不能挂载到其他卷，也不能与其他卷有硬链接。 Pod 中的每个容器必须独立地指定每个卷的挂载位置。

当前，k8s支持多种类型的Volume，包括：

[awsElasticBlockStore](https://kubernetes.io/docs/concepts/storage/volumes/#awselasticblockstore), [azureDisk](https://kubernetes.io/docs/concepts/storage/volumes/#azuredisk), [azureFile](https://kubernetes.io/docs/concepts/storage/volumes/#azurefile), [cephfs](https://kubernetes.io/docs/concepts/storage/volumes/#cephfs), [cinder](https://kubernetes.io/docs/concepts/storage/volumes/#cinder), [configMap](https://kubernetes.io/docs/concepts/storage/volumes/#configmap), [csi](https://kubernetes.io/docs/concepts/storage/volumes/#csi), [downwardAPI](https://kubernetes.io/docs/concepts/storage/volumes/#downwardapi), [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir), [fc (fibre channel)](https://kubernetes.io/docs/concepts/storage/volumes/#fc), [flexVolume](https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume), [flocker](https://kubernetes.io/docs/concepts/storage/volumes/#flocker), [gcePersistentDisk](https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk), [gitRepo (deprecated)](https://kubernetes.io/docs/concepts/storage/volumes/#gitrepo), [glusterfs](https://kubernetes.io/docs/concepts/storage/volumes/#glusterfs), [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath), [iscsi](https://kubernetes.io/docs/concepts/storage/volumes/#iscsi), [local](https://kubernetes.io/docs/concepts/storage/volumes/#local), [rbd](https://kubernetes.io/docs/concepts/storage/volumes/#rbd), [nfs](https://kubernetes.io/docs/concepts/storage/volumes/#nfs), [persistentVolumeClaim](https://kubernetes.io/docs/concepts/storage/volumes/#persistentvolumeclaim), [projected](https://kubernetes.io/docs/concepts/storage/volumes/#projected), [portworxVolume](https://kubernetes.io/docs/concepts/storage/volumes/#portworxvolume), [quobyte](https://kubernetes.io/docs/concepts/storage/volumes/#quobyte), [rbd](https://kubernetes.io/docs/concepts/storage/volumes/#rbd), [scaleIO](https://kubernetes.io/docs/concepts/storage/volumes/#scaleio), [storageos](https://kubernetes.io/docs/concepts/storage/volumes/#storageos),  [vsphereVolume](https://kubernetes.io/docs/concepts/storage/volumes/#vspherevolume)

以**emptyDir**为例：

当 Pod 指定到某个节点上时，首先创建的是一个 `emptyDir` 卷，并且只要 Pod 在该节点上运行，卷就一直存在。 就像它的名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 `emptyDir` 卷的路径可能相同也可能不同，但是这些容器都可以读写 `emptyDir` 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，`emptyDir` 卷中的数据也会永久删除。需要注意的是，容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃时 `emptyDir` 卷中的数据是安全的。

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
```

## 13 Persistent Volume 与 Persistent Volume Claim

### Persistent Volume (PV)

Volume是被定义在Pod上的，属于计算资源的一部分，而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。

比如在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中划出一个“网盘”并挂接到虚拟机上。Persistent Volume（PV）和与之相关联的Persistent Volume Claim（PVC）也起到了类似的作用。

**与Volume的区别**：PV可以被理解成Kubernetes集群中的某个网络存储对应的一块存储，它与Volume类似，但有以下区别：

- PV只能是网络存储，不属于任何Node，但可以在每个Node上访问。
- PV并不是被定义在Pod上的，而是独立于Pod之外定义的。
- PV目前支持的类型包括：gcePersistentDisk、AWSElasticBlockStore、AzureFile、AzureDisk、FC（Fibre Channel）、Flocker、NFS、iSCSI、RBD（Rados Block Device）、CephFS、Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMwarePhoton、Portworx Volumes、ScaleIO Volumes和HostPath（仅供单机测试）。

例子：

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /somepath
    server: 172.17.0.2
```

PV的accessModes属性，目前有以下类型：
·  ReadWriteOnce：读写权限，并且只能被单个Node挂载。
·  ReadOnlyMany：只读权限，允许被多个Node挂载。
·  ReadWriteMany：读写权限，允许被多个Node挂载

其次，PV是有状态的对象，它的状态有以下几种。
·   Available：空闲状态。
·   Bound：已经绑定到某个PVC上。
·   Released：对应的PVC已经被删除，但资源还没有被集群收回。
·   Failed：PV自动回收失败。

### Persistent Volume Claim (PVC)

如果某个Pod想申请某种类型的PV，则首先需要定义一个PersistentVolumeClaim对象，例如：

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
```

然后，在Pod的Volume定义中引用上述PVC即可，例如：

```
VOlumes :
  - name: mypd
    persistentVolumeClaim:
    claimName: myclaim
```

## 14 Namespace —— 命名空间

Namespace在很多情况下用于实现多租户的资源隔离。

Namespace通过将集群内部的资源对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。

Kubernetes集群在启动后会创建一个名为**default**的Namespace，通过kubectl可以查看：

```
$ kubectl get namespaces
NAME		LABELS		STATUS
default		<none>		Active
```

如果不特别指明Namespace，则用户创建的Pod、RC、Service都将被系统创建到这个默认的名为default的Namespace中。

Namespace的定义方式可以是这样的：

```
apiVersion: v1
kind: Namespace
metadata :
  name: development
```

然后在创建资源对象时就可以使用定义好的Namespace，例如

```
apiVersion: v1
kind: Pod
metadata :
  name: busybox
  namespace: development
spec:
  containers:
    - image: busybox 
  command:
    - Sleep
    - "3600"
  name: busybox
```

当给每个租户创建一个Namespace来实现多租户的资源隔离时，还能结合Kubernetes的资源配额管理，限定不同租户能占用的资源，例如CPU使用量、内存使用量等。

## 15 ConfigMap

Docker通过将程序、依赖库、数据及配置文件“打包固化”到一个不变的镜像文件中的做法，解决了应用的部署的难题，但这同时带来了棘手的问题，即配置文件中的参数在运行期如何修改的问题。我们不可能在启动Docker容器后再修改容器里的配置文件，然后用新的配置文件重启容器里的用户主进程。为了解决这个问题，Docker提供了两种方式：

- 在运行时通过容器的环境变量来传递参数；
- 通过Docker Volume将容器外的配置文件映射到容器内。

附：

[Docker容器修改配置文件 - CSDN博客](https://blog.csdn.net/qq_41218849/article/details/82821459)

[Docker修改已停止运行容器配置文件 - CSDN博客](https://blog.csdn.net/bacteriumX/article/details/87691770)

在大多数情况下，后一种方式更适用，因为多数应用通常从一个或多个配置文件中读取参数。但这种方式也有明显的缺陷：我们必须在目标主机上先创建好对应的配置文件，然后才能映射到容器里。该问题在分布式情况下变得更为严重，因为无论采用哪种方式，写入（修改）多台服务器上的某个指定文件，并确保这些文件保持一致，都是一个很难完成的目标。

Kubernetes的解决方法如下：

先把所有的配置项都当作key-value字符串，这些配置项可以作为Map表中的一个项，整个Map的数据可以被持久化存储在Kubernetes的Etcd数据库中。然后提供API以方便Kubernetes相关组件或客户应用CRUD操作这些数据，这个Map就是Kubernetes ConfigMap资源对象。

然后，Kubernetes提供了一种内建机制，将存储在etcd中的ConfigMap通过Volume映射的方式自动映射成所有目标Pod内的配置文件。当ConfigMap的数据被修改时，映射到Pod中的配置文件也会自动更新。

## 16 kubectl

**官方文档**：[Kubectl Reference Docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands), [kubectl CLI - Kubernetes](https://kubernetes.io/docs/reference/kubectl)

Kubectl 是一个用于控制 Kubernetes 集群的**命令行工具**。

`kubectl` 在 `$HOME/.kube` 目录中查找一个名为 config 的文件。用户可以通过设置环境变量 `KUBECONFIG` 或设置 [`--kubeconfig`](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) 参数的方式指定其它 [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) 文件。